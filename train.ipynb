{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "datapath = 'data/tcd ml 2019-20 income prediction training (with labels).csv'\n",
    "\n",
    "# source: https://data.worldbank.org/indicator/NY.GDP.MKTP.CD\n",
    "# The top rows were removed to aid in parsing\n",
    "gdp = pd.read_csv('data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_180634.csv', skiprows=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for ml algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def clean_country(country):\n",
    "    try:\n",
    "        return {\n",
    "            'Laos': 'Lao PDR',\n",
    "            'Kyrgyzstan': 'Kyrgyz Republic',\n",
    "            'Slovakia': 'Slovak Republic',\n",
    "            'Congo': 'Congo, Rep.',\n",
    "            'DR Congo': 'Congo, Dem. Rep.',\n",
    "            # Nothing political intended here, trying to be realistic about income\n",
    "            'State of Palestine': 'Jordan',\n",
    "            'Syria': 'Syrian Arab Republic',\n",
    "            'Gambia': 'Gambia, The',\n",
    "            'North Korea': 'Korea, Dem. People’s Rep.',\n",
    "            'South Korea': 'Korea, Rep.',\n",
    "            'Côte d\\'Ivoire': 'Cote d\\'Ivoire',\n",
    "            'Venezuela': 'Venezuela, RB',\n",
    "            'Yemen': 'Yemen, Rep.',\n",
    "            'Brunei': 'Brunei Darussalam',\n",
    "            'Micronesia': 'Micronesia, Fed. Sts.',\n",
    "            'Bahamas': 'Bahamas, The',\n",
    "            'Saint Lucia': 'St. Lucia',\n",
    "            'Czechia': 'Czech Republic',\n",
    "            'Sao Tome & Principe': 'Sao Tome and Principe',\n",
    "        }[country]\n",
    "    except KeyError:\n",
    "        return country\n",
    "\n",
    "def get_index_equiv(year):\n",
    "    return year - 1956\n",
    "\n",
    "def increment_year(year, subtract=True):\n",
    "    if subtract == True:\n",
    "        return year - 1\n",
    "    else:\n",
    "        return year + 1\n",
    "\n",
    "def country_to_gdp(data):\n",
    "    countries = pd.Series.to_dict(data['Country'])\n",
    "    years = pd.Series.to_dict(data['Year of Record'])\n",
    "    assert(len(countries) == len(years))\n",
    "    \n",
    "    return_val = {}\n",
    "    for i in range(len(countries)):\n",
    "        try:\n",
    "            gdpval = np.nan\n",
    "            year = years[i]\n",
    "            subtract = True\n",
    "            while np.isnan(gdpval):\n",
    "                gdpval = gdp.loc[\n",
    "                    gdp['Country Name'] == \n",
    "                                 clean_country(countries[i])].iloc[:, \n",
    "                                                                   get_index_equiv(int(year))\n",
    "                                                                  ].item()\n",
    "                year = increment_year(year, subtract)\n",
    "                if year < 1960:\n",
    "                    if countries[i] == 'North Korea':\n",
    "                        # There isn't World Bank data for NK, let's assume it's USD32 Billion\n",
    "                        gdpval = float(32000000000)\n",
    "                    else:\n",
    "                        subtract = False\n",
    "                        year = years[i]\n",
    "                elif (year > 2017) and (subtract == False):\n",
    "                    raise IndexError('Could not find GDP: ', countries[i], years[i])\n",
    "            return_val[i] = gdpval\n",
    "        except ValueError:\n",
    "            print('Country Error, check : clean_country() -> ', countries[i], years[i])\n",
    "            raise\n",
    "#     print(return_val)\n",
    "#     raise KeyboardInterrupt()\n",
    "    return pd.DataFrame(return_val, index=[0]).transpose()\n",
    "\n",
    "def shorten_jobs(jobs, length=8, thresh=80):\n",
    "    return_val = {}\n",
    "    for key in jobs:\n",
    "        if jobs[key] > thresh:\n",
    "            try:\n",
    "                new_key = key[:length]\n",
    "            except TypeError:\n",
    "                new_key = 'unkn'\n",
    "        else:\n",
    "            new_key = 'unkn'\n",
    "        return_val[key] = new_key\n",
    "    \n",
    "    # Make sure that there aren't any duplicates\n",
    "    for key in return_val:\n",
    "        try:\n",
    "            return_val[return_val[key]]\n",
    "            return_val[key] = 'unkn'\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    return return_val\n",
    "\n",
    "def one_hot_encode(data, column):\n",
    "    degree_encoder = LabelBinarizer()\n",
    "    degree_encoder.fit(data[column])\n",
    "    pk.dump(degree_encoder, open(column.replace(' ', '_') + '.pkl', mode='wb'))\n",
    "    transformed = degree_encoder.transform(data[column])\n",
    "    ohe_df = pd.DataFrame(transformed)\n",
    "    return pd.concat([data, ohe_df], axis=1).drop([column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path, \n",
    "                 Instance_drop, \n",
    "                 Year_of_Record_drop, \n",
    "                 Gender_drop, \n",
    "                 Age_drop, \n",
    "                 Country_drop, \n",
    "                 Size_of_City_drop, \n",
    "                 Profession_drop, \n",
    "                 University_Degree_drop, \n",
    "                 Wears_Glasses_drop, \n",
    "                 Hair_Color_drop, \n",
    "                 Body_Height_cm_drop,\n",
    "                 prof_len=4,\n",
    "                 prof_cutoff=80,\n",
    "):\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.fillna(value=0)\n",
    "\n",
    "    ## 'Instance'\n",
    "    if Instance_drop == True:\n",
    "        data = data.drop(labels='Instance', axis=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## 'Year of Record'\n",
    "    if Year_of_Record_drop == True:\n",
    "        data = data.drop(labels='Year of Record', axis=1)\n",
    "    else:\n",
    "        data = data.replace(to_replace={'Year of Record': {\n",
    "                0: 2018.0,\n",
    "                np.nan: 2018.0,\n",
    "        }})\n",
    "\n",
    "    ## 'Gender'\n",
    "    if Gender_drop == True:\n",
    "        data = data.drop(labels='Gender', axis=1)\n",
    "    else:\n",
    "        data = data.replace(to_replace={'Gender': {\n",
    "                'male': 1,\n",
    "                'female': -1,\n",
    "                'other': 0,\n",
    "                'unknown': 0,\n",
    "                '0': 0,\n",
    "                np.nan: 0,\n",
    "        }})\n",
    "\n",
    "    ## 'Age'\n",
    "    if Age_drop == True:\n",
    "        data = data.drop(labels='Age', axis=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## 'Country'\n",
    "    # Replace country with its GDP\n",
    "    if Country_drop == True:\n",
    "        data = data.drop(labels='Country', axis=1)\n",
    "    else:\n",
    "        data['Country'] = country_to_gdp(data)\n",
    "\n",
    "    ## 'Size of City'\n",
    "    if Size_of_City_drop == True:\n",
    "        data = data.drop(labels='Size of City', axis=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## 'Profession'\n",
    "    if Profession_drop == True:\n",
    "        data = data.drop(labels='Profession', axis=1)\n",
    "    else:\n",
    "        # shorten labels\n",
    "        new_jobs = {'Profession': \n",
    "                    shorten_jobs(\n",
    "                        pd.Series.to_dict(data['Profession'].value_counts()), \n",
    "                        prof_len,\n",
    "                        prof_cutoff\n",
    "                    )}\n",
    "        pk.dump(new_jobs, open('jobs.pkl', mode='wb'))\n",
    "        data = data.replace(to_replace=new_jobs)\n",
    "\n",
    "        # one-hot encode \n",
    "        data = one_hot_encode(data, 'Profession')\n",
    "\n",
    "    ## 'University Degree'\n",
    "    if University_Degree_drop == True:\n",
    "        data = data.drop(labels='University Degree', axis=1)\n",
    "    else:\n",
    "        data = data.replace(to_replace={'University Degree': {\n",
    "                '0': 'No',\n",
    "                0: 'No',\n",
    "                np.nan: 'No',\n",
    "        }})\n",
    "\n",
    "        data = one_hot_encode(data, 'University Degree')\n",
    "\n",
    "    ## 'Wears Glasses'\n",
    "    if Wears_Glasses_drop == True:\n",
    "        data = data.drop(labels='Wears Glasses', axis=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## 'Hair Color'\n",
    "    if Hair_Color_drop == True:\n",
    "        data = data.drop(labels='Hair Color', axis=1)\n",
    "    else:\n",
    "        data = data.replace(to_replace={'Hair Color': {\n",
    "                np.nan: 'Unknown',\n",
    "                '0': 'Unknown',\n",
    "                0: 'Unknown',\n",
    "        }})\n",
    "        data = one_hot_encode(data, 'Hair Color')\n",
    "\n",
    "    ## 'Body Height [cm]'\n",
    "    if Body_Height_cm_drop == True:\n",
    "        data = data.drop(labels='Body Height [cm]', axis=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## 'Income in EUR'\n",
    "    # nothing to do here\n",
    "    \n",
    "    return data\n",
    "\n",
    "def split__scale_data(data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = data.drop(columns=['Income in EUR'])#.to_numpy()\n",
    "    y = data.loc[:, 'Income in EUR']#.to_numpy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    pk.dump(scaler, open('scaler.pkl', mode='wb'))\n",
    "\n",
    "    X = scaler.transform(X)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=12120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: `item` has been deprecated and will be removed in a future version\n"
     ]
    }
   ],
   "source": [
    "data = process_data(\n",
    "    datapath, \n",
    "    \n",
    "    Instance_drop           = True,\n",
    "    Year_of_Record_drop     = False,\n",
    "    Gender_drop             = False,\n",
    "    Age_drop                = False,\n",
    "    Country_drop            = False,\n",
    "    Size_of_City_drop       = True,\n",
    "    Profession_drop         = True,\n",
    "    University_Degree_drop  = False, # potentially drop\n",
    "    Wears_Glasses_drop      = True,\n",
    "    Hair_Color_drop         = True,\n",
    "    Body_Height_cm_drop     = False,\n",
    "    \n",
    "    prof_len                = 8,\n",
    "    prof_cutoff             = 80,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = split__scale_data(data)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "func = RandomForestRegressor(\n",
    "    n_estimators=10, \n",
    "    n_jobs=-1,\n",
    "    random_state=2645, # for consistency between runs\n",
    "    verbose=0\n",
    ")\n",
    "func.fit(X_train, y_train)\n",
    "print('Score: ', func.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression # N/A\n",
    "# from sklearn.tree import DecisionTreeRegressor # 0.5144117272297519\n",
    "# from sklearn.ensemble import RandomForestRegressor # 0.6767255620150188\n",
    "# from sklearn.neighbors import KNeighborsRegressor # -0.15649302917615013\n",
    "# from sklearn.neural_network import MLPRegressor # 0.009245706360739738\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.svm import SVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(func, open('predictor.pkl', mode='wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk.dump(func, open('predictor.pkl', mode='wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
